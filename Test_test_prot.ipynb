{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import tensorflow as tf\r\n",
    "import os\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, classification_report\r\n",
    "from itertools import permutations,combinations,product\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.linear_model import SGDClassifier"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# cia yra generatorius. Tai yra funkcija kuri paimą tam tikrą eilučių kiekį \"row_count\" is csv. failo kad neuzkrautum atminties.\r\n",
    "# Taip pat, visa informacija informacia yra sudedam yra tam tikru \"batchu\" kiekius. Tai yra daroma del to, kad mokymas butu geresnis\r\n",
    "def generator(File, batch_size):\r\n",
    "    # paimamu eiluciu skaicius\r\n",
    "    row_count = 50000\r\n",
    "    # suskaiciuojama kiek bus naudojama eiluciu per epocha. Epocha yra vienas ciklas per kuri yra idedami visos eilutes.\r\n",
    "    samples_per_epoch = sum(len(row) for row in pd.read_csv(File,chunksize=row_count))\r\n",
    "    # batchu kiekis per epocha\r\n",
    "    number_of_batches = samples_per_epoch/batch_size\r\n",
    "    # suskaiciuojama po kiek reikia suskirstyti chunkus, kad is ju gautusi sveikas batchu skaicius  \r\n",
    "    chunk_size_divided = int(row_count/batch_size)\r\n",
    "    # pradedamas dalinimas\r\n",
    "    counter = 0\r\n",
    "    while 1:\r\n",
    "        # isemamas chunkas is .csv failo. \r\n",
    "        # pvz: eluciu kiekis yra 120 chunku dydis yra 50. Tai is chunko bus isemami 3 chunkai su dydziais 50,50,20\r\n",
    "        for chunk in pd.read_csv(File, chunksize=chunk_size_divided*batch_size):\r\n",
    "            # jei chunko dydis yra toks pats koks yra nustatyta. \r\n",
    "            if len(chunk) == chunk_size_divided*batch_size:\r\n",
    "                # suskirsto chunku stulpelius \r\n",
    "                X_data = chunk.iloc[:, 0:-1].values\r\n",
    "                y_data = chunk.iloc[:, -1].values\r\n",
    "\r\n",
    "                # chunkai suskirstomi i batchus\r\n",
    "                for i in range(chunk_size_divided):\r\n",
    "\r\n",
    "                    X_batch = np.array(X_data[batch_size*i:batch_size*(i+1)]).astype('float32')\r\n",
    "                    y_batch = np.array(y_data[batch_size*i:batch_size*(i+1)]).astype('float32')\r\n",
    "                    counter += 1\r\n",
    "                    # batchai idedami i ann\r\n",
    "                    yield X_batch,y_batch\r\n",
    "            # jeigu failo pabaiga ir chunkas yra mazesnis negu numatyta\r\n",
    "            else:\r\n",
    "                # jei chunkas nesuskirsto tolygiai i batchus. paimamas likutis\r\n",
    "                if len(chunk)/batch_size == int(len(chunk)/batch_size):\r\n",
    "                    chunk_size_divided_ending = int(len(chunk)/batch_size)\r\n",
    "                else:\r\n",
    "                    chunk_size_divided_ending = int(len(chunk)/batch_size) + 1\r\n",
    "\r\n",
    "                for i in range(chunk_size_divided_ending):\r\n",
    "                    X_batch = np.array(X_data[batch_size*i:batch_size*(i+1)]).astype('float32')\r\n",
    "                    y_batch = np.array(y_data[batch_size*i:batch_size*(i+1)]).astype('float32')\r\n",
    "                    counter += 1\r\n",
    "                    yield X_batch,y_batch\r\n",
    "            #restart counter to yeild data in the next epoch as well\r\n",
    "\r\n",
    "        if counter >= number_of_batches:\r\n",
    "            counter = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# sudaromas ann modelis. siuo metu 3 layeriai po 5 nodes\r\n",
    "ann = tf.keras.models.Sequential()\r\n",
    "ann.add(tf.keras.layers.Dense(units=5, activation='relu'))\r\n",
    "ann.add(tf.keras.layers.Dense(units=5, activation='relu'))\r\n",
    "ann.add(tf.keras.layers.Dense(units=5, activation='relu'))\r\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\r\n",
    "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\r\n",
    "\r\n",
    "# suvedamas batcho dydis\r\n",
    "batch_size = 8\r\n",
    "\r\n",
    "# suskaiciuojama kiek eiluciu turi datasetas\r\n",
    "# .csv failai jau yra transformuoti. dabar as naudojau tik 02-14-2018.cvs dataseta.\r\n",
    "# failas buvo tranformuotas su Transform.ipynb\r\n",
    "rows_train = sum(len(row) for row in pd.read_csv(r\"Temp_Data\\train_data.csv\",chunksize=50000))\r\n",
    "rows_test = sum(len(row) for row in pd.read_csv(r\"Temp_Data\\test_data.csv\",chunksize=50000))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# pradedame mokinti ann\r\n",
    "ann.fit_generator(\r\n",
    "    generator(r\"Temp_Data\\train_data.csv\", batch_size),\r\n",
    "    epochs=2,\r\n",
    "    steps_per_epoch = rows_train/batch_size,\r\n",
    "    validation_data = generator(r\"Temp_Data\\test_data.csv\", batch_size),\r\n",
    "    validation_steps = rows_test/batch_size\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\lukut\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "117534/117534 [==============================] - 165s 1ms/step - loss: 0.5025 - accuracy: 0.7989 - val_loss: 2099.9407 - val_accuracy: 0.6350\n",
      "Epoch 2/5\n",
      "117534/117534 [==============================] - 161s 1ms/step - loss: 0.4560 - accuracy: 0.8068 - val_loss: 4090.2410 - val_accuracy: 0.6350\n",
      "Epoch 3/5\n",
      "117534/117534 [==============================] - 155s 1ms/step - loss: 0.4623 - accuracy: 0.8079 - val_loss: 4135.0674 - val_accuracy: 0.6350\n",
      "Epoch 4/5\n",
      "117534/117534 [==============================] - 183s 2ms/step - loss: 0.4585 - accuracy: 0.8079 - val_loss: 3926.8071 - val_accuracy: 0.6351\n",
      "Epoch 5/5\n",
      "117534/117534 [==============================] - 232s 2ms/step - loss: 0.4595 - accuracy: 0.8057 - val_loss: 2186.2305 - val_accuracy: 0.6350\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1babe22f970>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit"
  },
  "interpreter": {
   "hash": "988ccdee98c6f280c02691a59536500eb11bb1fd9b0fe1ccc3ec258f0162fdd8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}